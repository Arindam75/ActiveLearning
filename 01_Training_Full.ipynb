{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cd9bbf-b3b0-4cc1-9fbf-1497140d7358",
   "metadata": {},
   "source": [
    "### Get the Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fc46e-7a54-42f4-a9f8-9d0a91873b03",
   "metadata": {},
   "source": [
    "If you haven't already, please download the dogs-vs-cats.zip from __[here](https://www.kaggle.com/competitions/dogs-vs-cats/data)__ and extract only the train.zip ( do not extract it's contents yet ) as a zipfile into the current project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78a8b36-3cd6-4ef7-8317-25fbecb8c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a16801-8749-431d-b5ad-cc0ff57dbce7",
   "metadata": {},
   "source": [
    "If the train.zip is available in the current folder, then the code below extracts its contents into a  subfolder called __train__. Its contents are then mached with __trainfiles.txt__ <br> This is to make sure that you have all the required 25000 image files for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e84a52-d788-4eb8-a9a5-2504014b71eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Train Files Found, please proceed\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(),\"train\")\n",
    "f = open(\"trainfiles.txt\", \"r\")\n",
    "trainfiles = f.read().split(\",\")\n",
    "target_zip = \"train.zip\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    #if train folder exists, check all file names with trainfiles.txt \n",
    "    avlblfiles = os.listdir(data_path)\n",
    "    chk_data = [file for file in trainfiles if file not in avlblfiles]\n",
    "    \n",
    "    if len(chk_data)==0:\n",
    "        \n",
    "        unzip = False\n",
    "        print(\"All Train Files Found, please proceed\")\n",
    "    else:\n",
    "        \n",
    "        print(\"Some train files are missing\")\n",
    "                \n",
    "except FileNotFoundError:\n",
    "    \n",
    "    unzip = True\n",
    "    print(\"Folder train not found, attempting unzip\")\n",
    "    \n",
    "#If train folder is not found, look for train.zip and\n",
    "#unzip its contents into a folder train\n",
    "    \n",
    "if unzip:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        with zipfile.ZipFile(target_zip) as zip_file:\n",
    "            print(\"Unzipping data\")\n",
    "            zip_file.extractall()\n",
    "        print(\"Train Data unzipped, please proceed\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        \n",
    "        print(\"File train.zip not found, please download from Kaggle \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49abea-fed4-48a4-8335-1d7a526195e3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246de192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.image import resize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.metrics import  Recall, CategoricalAccuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "\n",
    "#A custom library for helper functions\n",
    "from src.helper import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991cbbd-adaf-459d-bdd9-555950de6a5c",
   "metadata": {},
   "source": [
    "### Build Datasets\n",
    "\n",
    "The dataset is one large collection of images of cats and dogs. The label can be identified from the file name. We load all the 25000 file names and the corresponding labels into an array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afae67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={'cat':0,'dog':1}\n",
    "dataset=np.array([(os.path.join(data_path,i),label_dict[i.split('.')[0]]) for i in os.listdir(data_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a133272-8d8a-4eb1-b9d9-c5136cea4367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C:\\\\Users\\\\arind\\\\Documents\\\\ActiveLearning\\\\train\\\\cat.0.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\ActiveLearning\\\\train\\\\cat.1.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\ActiveLearning\\\\train\\\\cat.10.jpg',\n",
       "        '0']], dtype='<U59')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32088349-db6b-4fa3-8020-b51918a9b699",
   "metadata": {},
   "source": [
    "We assign all the filenames to X and the labels to y. Since we have two classes , we use the keras method to one-hot encode the labels. <br> Since the files aren't shuffled , with all the cat files appearing first , followed by the dog, we shuffle X and y. <br>\n",
    "\n",
    "We reserve 10% of the samples for train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977c450b-da5e-47f1-900c-bee9837f61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset[::,0],dataset[::,1]\n",
    "y = y.astype(int)\n",
    "\n",
    "#One hot encode the labels\n",
    "y = to_categorical(y)\n",
    "\n",
    "#Shuffle the dataset\n",
    "p = np.random.permutation(len(X))\n",
    "X,y = X[p], y[p]\n",
    "\n",
    "#Strip off 10% samples for hold out test set\n",
    "test_idxs = np.random.choice(len(X), size=int(0.1*len(X)), replace=False, p=None)\n",
    "x_test, y_test = X[test_idxs],y[test_idxs]\n",
    "\n",
    "#Delete the test set samples from X,y \n",
    "X = np.delete(X, test_idxs)\n",
    "y = np.delete(y, test_idxs, axis = 0)\n",
    "\n",
    "#usual train-val split. We use 0.11 here just match the test set size to validation set.\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38b00e-bccb-4045-a2f6-7eb9f1fadbee",
   "metadata": {},
   "source": [
    "A quick check on the sample counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b4e7d3-dafd-43a4-9c63-ec96ef2e204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in Training set: 20025\n",
      "Samples in Validation set: 2475\n",
      "Samples in Test set: 2500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Samples in Training set: {x_train.shape[0]}\")\n",
    "print(f\"Samples in Validation set: {x_val.shape[0]}\")\n",
    "print(f\"Samples in Test set: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c59b12-3dee-4451-9853-0b70794a0ab2",
   "metadata": {},
   "source": [
    "A quick check for data imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927babd6-b497-4b73-a491-b3f8f34b96cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([10085,  9940], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1211, 1289], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1204, 1271], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for i in [y_train, y_test, y_val]:\n",
    "    print(np.unique(i, return_counts = True, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7292f8-5cf5-4c9d-888d-1712fec486ad",
   "metadata": {},
   "source": [
    "We use the helper function to convert the data into tensorflow dataset objects. Note that , the __repeat__ flag needs to be set only for the train set , which by default is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5f8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The buid_dataset is a cutom function that returns tensor batches\n",
    "\n",
    "val_dataset=build_dataset(x_val,y_val,repeat=False,batch=256)\n",
    "test_dataset=build_dataset(x_test,y_test,repeat=False,batch=256)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "STEPS_PER_EPOCH=len(x_train)/BATCH_SIZE\n",
    "\n",
    "train_dataset=build_dataset(x_train,y_train,batch=BATCH_SIZE)\n",
    "input_shape=train_dataset.element_spec[0].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d224b67-e500-467b-a2d8-92b04cad264c",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "This is quite standard. We use the helper functions to build a simple neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7708bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 30, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,866\n",
      "Trainable params: 684,162\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=simple_model(input_shape)\n",
    "model.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = Adam(),\n",
    "        metrics = CategoricalAccuracy()\n",
    "    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24bec8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=ModelCheckpoint(filepath='model/model_baseline.h5',\n",
    "                           monitor='val_loss',save_best_only=True,verbose=1)\n",
    "\n",
    "csv_logger=keras.callbacks.CSVLogger('logger/trainlog_baseline.csv',\n",
    "                                     separator=',',append=False)\n",
    "\n",
    "early_stopper=keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            min_delta=0.001,\n",
    "                                            restore_best_weights=True,\n",
    "                                            patience=10)\n",
    "\n",
    "callbacks_list=[checkpoint,early_stopper,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc49bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1247/1251 [============================>.] - ETA: 0s - loss: 0.6895 - categorical_accuracy: 0.6432\n",
      "Epoch 1: val_loss improved from inf to 0.65976, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 19s 10ms/step - loss: 0.6889 - categorical_accuracy: 0.6434 - val_loss: 0.6598 - val_categorical_accuracy: 0.6509\n",
      "Epoch 2/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.5401 - categorical_accuracy: 0.7261\n",
      "Epoch 2: val_loss improved from 0.65976 to 0.59598, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 12s 10ms/step - loss: 0.5400 - categorical_accuracy: 0.7263 - val_loss: 0.5960 - val_categorical_accuracy: 0.6683\n",
      "Epoch 3/200\n",
      "1248/1251 [============================>.] - ETA: 0s - loss: 0.4902 - categorical_accuracy: 0.7590\n",
      "Epoch 3: val_loss improved from 0.59598 to 0.49338, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 12s 10ms/step - loss: 0.4903 - categorical_accuracy: 0.7589 - val_loss: 0.4934 - val_categorical_accuracy: 0.7620\n",
      "Epoch 4/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.4567 - categorical_accuracy: 0.7845\n",
      "Epoch 4: val_loss did not improve from 0.49338\n",
      "1251/1251 [==============================] - 12s 10ms/step - loss: 0.4563 - categorical_accuracy: 0.7847 - val_loss: 0.4973 - val_categorical_accuracy: 0.7503\n",
      "Epoch 5/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.4280 - categorical_accuracy: 0.8060\n",
      "Epoch 5: val_loss did not improve from 0.49338\n",
      "1251/1251 [==============================] - 12s 10ms/step - loss: 0.4279 - categorical_accuracy: 0.8060 - val_loss: 0.7021 - val_categorical_accuracy: 0.7358\n",
      "Epoch 6/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.4046 - categorical_accuracy: 0.8141\n",
      "Epoch 6: val_loss improved from 0.49338 to 0.36065, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 12s 10ms/step - loss: 0.4044 - categorical_accuracy: 0.8142 - val_loss: 0.3607 - val_categorical_accuracy: 0.8384\n",
      "Epoch 7/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.3840 - categorical_accuracy: 0.8272\n",
      "Epoch 7: val_loss did not improve from 0.36065\n",
      "1251/1251 [==============================] - 13s 10ms/step - loss: 0.3837 - categorical_accuracy: 0.8273 - val_loss: 0.3694 - val_categorical_accuracy: 0.8319\n",
      "Epoch 8/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.3593 - categorical_accuracy: 0.8440\n",
      "Epoch 8: val_loss did not improve from 0.36065\n",
      "1251/1251 [==============================] - 13s 10ms/step - loss: 0.3594 - categorical_accuracy: 0.8441 - val_loss: 0.3616 - val_categorical_accuracy: 0.8347\n",
      "Epoch 9/200\n",
      "1247/1251 [============================>.] - ETA: 0s - loss: 0.3481 - categorical_accuracy: 0.8455\n",
      "Epoch 9: val_loss did not improve from 0.36065\n",
      "1251/1251 [==============================] - 13s 10ms/step - loss: 0.3482 - categorical_accuracy: 0.8453 - val_loss: 0.3672 - val_categorical_accuracy: 0.8352\n",
      "Epoch 10/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.3503 - categorical_accuracy: 0.8473\n",
      "Epoch 10: val_loss did not improve from 0.36065\n",
      "1251/1251 [==============================] - 13s 10ms/step - loss: 0.3503 - categorical_accuracy: 0.8473 - val_loss: 0.3621 - val_categorical_accuracy: 0.8428\n",
      "Epoch 11/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.3275 - categorical_accuracy: 0.8591\n",
      "Epoch 11: val_loss improved from 0.36065 to 0.33583, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 13s 10ms/step - loss: 0.3274 - categorical_accuracy: 0.8591 - val_loss: 0.3358 - val_categorical_accuracy: 0.8545\n",
      "Epoch 12/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.3172 - categorical_accuracy: 0.8638\n",
      "Epoch 12: val_loss did not improve from 0.33583\n",
      "1251/1251 [==============================] - 13s 10ms/step - loss: 0.3171 - categorical_accuracy: 0.8637 - val_loss: 0.3643 - val_categorical_accuracy: 0.8432\n",
      "Epoch 13/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.3082 - categorical_accuracy: 0.8693\n",
      "Epoch 13: val_loss did not improve from 0.33583\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.3082 - categorical_accuracy: 0.8693 - val_loss: 0.6633 - val_categorical_accuracy: 0.6917\n",
      "Epoch 14/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.2956 - categorical_accuracy: 0.8745\n",
      "Epoch 14: val_loss improved from 0.33583 to 0.32429, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2957 - categorical_accuracy: 0.8744 - val_loss: 0.3243 - val_categorical_accuracy: 0.8651\n",
      "Epoch 15/200\n",
      "1247/1251 [============================>.] - ETA: 0s - loss: 0.2954 - categorical_accuracy: 0.8754\n",
      "Epoch 15: val_loss improved from 0.32429 to 0.31646, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2954 - categorical_accuracy: 0.8753 - val_loss: 0.3165 - val_categorical_accuracy: 0.8626\n",
      "Epoch 16/200\n",
      "1248/1251 [============================>.] - ETA: 0s - loss: 0.2903 - categorical_accuracy: 0.8758\n",
      "Epoch 16: val_loss did not improve from 0.31646\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2900 - categorical_accuracy: 0.8758 - val_loss: 0.3557 - val_categorical_accuracy: 0.8509\n",
      "Epoch 17/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.2733 - categorical_accuracy: 0.8859\n",
      "Epoch 17: val_loss improved from 0.31646 to 0.29729, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2733 - categorical_accuracy: 0.8859 - val_loss: 0.2973 - val_categorical_accuracy: 0.8695\n",
      "Epoch 18/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2718 - categorical_accuracy: 0.8866\n",
      "Epoch 18: val_loss did not improve from 0.29729\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2716 - categorical_accuracy: 0.8866 - val_loss: 0.3400 - val_categorical_accuracy: 0.8610\n",
      "Epoch 19/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2593 - categorical_accuracy: 0.8921\n",
      "Epoch 19: val_loss did not improve from 0.29729\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2592 - categorical_accuracy: 0.8920 - val_loss: 0.3302 - val_categorical_accuracy: 0.8549\n",
      "Epoch 20/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2477 - categorical_accuracy: 0.8972\n",
      "Epoch 20: val_loss did not improve from 0.29729\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2480 - categorical_accuracy: 0.8972 - val_loss: 0.3429 - val_categorical_accuracy: 0.8570\n",
      "Epoch 21/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2550 - categorical_accuracy: 0.8944\n",
      "Epoch 21: val_loss improved from 0.29729 to 0.28725, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2551 - categorical_accuracy: 0.8944 - val_loss: 0.2873 - val_categorical_accuracy: 0.8743\n",
      "Epoch 22/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.2519 - categorical_accuracy: 0.8929\n",
      "Epoch 22: val_loss did not improve from 0.28725\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2517 - categorical_accuracy: 0.8930 - val_loss: 0.2885 - val_categorical_accuracy: 0.8784\n",
      "Epoch 23/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.2362 - categorical_accuracy: 0.9038\n",
      "Epoch 23: val_loss did not improve from 0.28725\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2361 - categorical_accuracy: 0.9039 - val_loss: 0.2942 - val_categorical_accuracy: 0.8760\n",
      "Epoch 24/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.2316 - categorical_accuracy: 0.9070\n",
      "Epoch 24: val_loss improved from 0.28725 to 0.28144, saving model to model\\model_baseline.h5\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2315 - categorical_accuracy: 0.9070 - val_loss: 0.2814 - val_categorical_accuracy: 0.8861\n",
      "Epoch 25/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.2428 - categorical_accuracy: 0.8987\n",
      "Epoch 25: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2428 - categorical_accuracy: 0.8987 - val_loss: 0.3755 - val_categorical_accuracy: 0.8537\n",
      "Epoch 26/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2325 - categorical_accuracy: 0.9059\n",
      "Epoch 26: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2321 - categorical_accuracy: 0.9061 - val_loss: 0.3203 - val_categorical_accuracy: 0.8788\n",
      "Epoch 27/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.2424 - categorical_accuracy: 0.9011\n",
      "Epoch 27: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2424 - categorical_accuracy: 0.9011 - val_loss: 0.2912 - val_categorical_accuracy: 0.8784\n",
      "Epoch 28/200\n",
      "1248/1251 [============================>.] - ETA: 0s - loss: 0.2259 - categorical_accuracy: 0.9074\n",
      "Epoch 28: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2260 - categorical_accuracy: 0.9074 - val_loss: 0.3034 - val_categorical_accuracy: 0.8780\n",
      "Epoch 29/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.2254 - categorical_accuracy: 0.9087\n",
      "Epoch 29: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 15s 12ms/step - loss: 0.2252 - categorical_accuracy: 0.9087 - val_loss: 0.3164 - val_categorical_accuracy: 0.8695\n",
      "Epoch 30/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.2152 - categorical_accuracy: 0.9123\n",
      "Epoch 30: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2151 - categorical_accuracy: 0.9123 - val_loss: 0.3507 - val_categorical_accuracy: 0.8634\n",
      "Epoch 31/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.2019 - categorical_accuracy: 0.9201\n",
      "Epoch 31: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2018 - categorical_accuracy: 0.9202 - val_loss: 0.3413 - val_categorical_accuracy: 0.8683\n",
      "Epoch 32/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.2300 - categorical_accuracy: 0.9060\n",
      "Epoch 32: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2304 - categorical_accuracy: 0.9058 - val_loss: 0.3142 - val_categorical_accuracy: 0.8663\n",
      "Epoch 33/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.1964 - categorical_accuracy: 0.9197\n",
      "Epoch 33: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.1962 - categorical_accuracy: 0.9197 - val_loss: 0.3243 - val_categorical_accuracy: 0.8727\n",
      "Epoch 34/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.2042 - categorical_accuracy: 0.9176\n",
      "Epoch 34: val_loss did not improve from 0.28144\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2046 - categorical_accuracy: 0.9175 - val_loss: 0.3793 - val_categorical_accuracy: 0.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ff857f7c40>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=200,\n",
    "          validation_data=val_dataset,validation_steps=None,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89956a-fea4-434a-872c-cefc21a8278e",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "So we have trained a model using the full training set with 20000 samples. How does it perform on the test set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a3af6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model/model_baseline.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e6fddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "{'loss': 0.2882618308067322, 'categorical_accuracy': 0.8831999897956848}\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 100)\n",
    "print(model.evaluate(test_dataset, verbose=0,return_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd70778-2bb7-45e5-9292-73daa85167cc",
   "metadata": {},
   "source": [
    "### Measuring Uncertainties\n",
    "\n",
    "In this section we evaluate the three metrics to measure uncertainty. We use the formula to find out the prediction probabilities of the 10 test samples with most uncertainty.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6060fe5-b1aa-49ad-bcd4-657869a6c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 42ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_proba = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cea326-8f68-4b02-aad5-fa54c4e66d02",
   "metadata": {},
   "source": [
    "Now that we have the prediction probabilities of the entire test set, we can apply the formula to calculate the uncertainty metric and select the top 10 uncertain samples.<br> Let us start with Least Confidence or __LC__\n",
    "\n",
    "$$ LC_i = 1 - P_{imax}  $$\n",
    "\n",
    "P_imax is the maximum probability of the i_th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ed9e1be-843f-448f-bab3-eedda3e4d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49423876 0.50576127]\n",
      " [0.49471268 0.5052873 ]\n",
      " [0.50421554 0.4957844 ]\n",
      " [0.50365025 0.49634972]\n",
      " [0.503      0.49699995]\n",
      " [0.497582   0.50241804]\n",
      " [0.49760765 0.50239235]\n",
      " [0.49801224 0.50198776]\n",
      " [0.50127554 0.49872446]\n",
      " [0.5001126  0.49988738]]\n"
     ]
    }
   ],
   "source": [
    "#Calculate Least Confidence\n",
    "y_test_uncert = 1 - y_test_proba.max(axis=1)\n",
    "#Indices of the top 10 Least Confidence\n",
    "y_test_top_lc = np.argsort(y_test_uncert)[-10:]\n",
    "#Print the predictions for the top 10 least confidence\n",
    "print(y_test_proba[y_test_top_lc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd6a00-2b3a-4ccc-9d42-c76f7e0c1c80",
   "metadata": {},
   "source": [
    "Margin of confidence of a sample is given by the 1st and 2nd highest prediction probability of a sample\n",
    "\n",
    "$$ MC_i = P_{i1} - P_{i2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d737397c-a423-4e17-a85c-0687128b7677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5001126  0.49988738]\n",
      " [0.50127554 0.49872446]\n",
      " [0.49801224 0.50198776]\n",
      " [0.49760765 0.50239235]\n",
      " [0.497582   0.50241804]\n",
      " [0.503      0.49699995]\n",
      " [0.50365025 0.49634972]\n",
      " [0.50421554 0.4957844 ]\n",
      " [0.49471268 0.5052873 ]\n",
      " [0.49423876 0.50576127]]\n"
     ]
    }
   ],
   "source": [
    "part = np.partition(-y_test_proba, 1, axis=1)\n",
    "# margin calculation\n",
    "margin = - part[:, 0] + part[:, 1]\n",
    "# indices of the lowest margin scores\n",
    "y_test_least_mc = np.argsort(margin)[:10]\n",
    "#Print the predictions for the 10 least margins\n",
    "print(y_test_proba[y_test_least_mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4a9dc-6c49-4405-8186-a7fd67fba1c8",
   "metadata": {},
   "source": [
    "Finally entropy of the i_th sample is given by. Thankfully , we don't have to write the code for this calculation , as scipy provides a neat method called __entropy__ to do precisely that. \n",
    "\n",
    "$$ \\ E_{i} = \\sum_{n=1} p_{in} log   p_{in} \\ $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c55311b0-217b-4cb2-949f-2df395545b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49423876 0.50576127]\n",
      " [0.49471268 0.5052873 ]\n",
      " [0.50421554 0.4957844 ]\n",
      " [0.50365025 0.49634972]\n",
      " [0.503      0.49699995]\n",
      " [0.497582   0.50241804]\n",
      " [0.49760765 0.50239235]\n",
      " [0.49801224 0.50198776]\n",
      " [0.50127554 0.49872446]\n",
      " [0.5001126  0.49988738]]\n"
     ]
    }
   ],
   "source": [
    "#indices of the predictions with 10 largest entropies\n",
    "y_test_max_ents = np.argsort(entropy(y_test_proba.T))[-10:]\n",
    "#Print the 10 predictions with largest entropies\n",
    "print(y_test_proba[y_test_max_ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc0c53a-958f-4e54-b3d8-5b457b0cf1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

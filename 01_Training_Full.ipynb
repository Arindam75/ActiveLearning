{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cd9bbf-b3b0-4cc1-9fbf-1497140d7358",
   "metadata": {},
   "source": [
    "### Get the Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fc46e-7a54-42f4-a9f8-9d0a91873b03",
   "metadata": {},
   "source": [
    "If you haven't already, please download the dogs-vs-cats.zip from __[here](https://www.kaggle.com/competitions/dogs-vs-cats/data)__ and extract only the train.zip ( do not extract it's contents yet ) as a zipfile into the current project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d78a8b36-3cd6-4ef7-8317-25fbecb8c420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a16801-8749-431d-b5ad-cc0ff57dbce7",
   "metadata": {},
   "source": [
    "If the train.zip is avaiblable in the current folder, then the code below extracts its contents intoa  subfolder called __train__. Its contents are then mached with __trainfiles.txt__ <br> This is to make sure that you have all the required 25000 image files for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55e84a52-d788-4eb8-a9a5-2504014b71eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Train Files Found, please proceed\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.join(os.getcwd(),\"train\")\n",
    "f = open(\"trainfiles.txt\", \"r\")\n",
    "trainfiles = f.read().split(\",\")\n",
    "target_zip = \"train.zip\"\n",
    "\n",
    "try:\n",
    "    \n",
    "    #if train folder exists, check all file names with trainfiles.txt \n",
    "    avlblfiles = os.listdir(data_path)\n",
    "    chk_data = [file for file in trainfiles if file not in avlblfiles]\n",
    "    \n",
    "    if len(chk_data)==0:\n",
    "        \n",
    "        unzip = False\n",
    "        print(\"All Train Files Found, please proceed\")\n",
    "    else:\n",
    "        \n",
    "        print(\"Some train files are missing\")\n",
    "                \n",
    "except FileNotFoundError:\n",
    "    \n",
    "    unzip = True\n",
    "    print(\"Folder train not found, attempting unzip\")\n",
    "    \n",
    "#If train folder is not found, look for train.zip and\n",
    "#unzip its contents into a folder train\n",
    "    \n",
    "if unzip:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        with zipfile.ZipFile(target_zip) as zip_file:\n",
    "            print(\"Unzipping data\")\n",
    "            zip_file.extractall()\n",
    "        print(\"Train Data unzipped, please proceed\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        \n",
    "        print(\"File train.zip not found, please download from Kaggle \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee49abea-fed4-48a4-8335-1d7a526195e3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "246de192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.image import resize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.metrics import  Recall, CategoricalAccuracy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "\n",
    "#A custom library for helper functions\n",
    "from src.helper import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991cbbd-adaf-459d-bdd9-555950de6a5c",
   "metadata": {},
   "source": [
    "### Build Datasets\n",
    "\n",
    "The dataset is one large collection of images of cats and dogs. The label can be identified from the file name. We load all the 25000 file names and the corresponding labels into an array.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afae67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={'cat':0,'dog':1}\n",
    "dataset=np.array([(os.path.join(data_path,i),label_dict[i.split('.')[0]]) for i in os.listdir(data_path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a133272-8d8a-4eb1-b9d9-c5136cea4367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C:\\\\Users\\\\arind\\\\Documents\\\\Active_Learning\\\\train\\\\cat.0.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\Active_Learning\\\\train\\\\cat.1.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\Active_Learning\\\\train\\\\cat.10.jpg',\n",
       "        '0']], dtype='<U60')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32088349-db6b-4fa3-8020-b51918a9b699",
   "metadata": {},
   "source": [
    "We assign all the filenames to X and the labels to y. Since we have two classes , we use the keras method to one-hot encode the labels. <br> Since the files aren't shuffled , with all the cat files appearing first , followed by the dog, we shuffle X and y. <br>\n",
    "\n",
    "We reserve 10% of the samples for train, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "977c450b-da5e-47f1-900c-bee9837f61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset[::,0],dataset[::,1]\n",
    "y = y.astype(int)\n",
    "\n",
    "#One hot encode the labels\n",
    "y = to_categorical(y)\n",
    "\n",
    "#Shuffle the dataset\n",
    "p = np.random.permutation(len(X))\n",
    "X,y = X[p], y[p]\n",
    "\n",
    "#Strip off 10% samples for hold out test set\n",
    "test_idxs = np.random.choice(len(X), size=int(0.1*len(X)), replace=False, p=None)\n",
    "x_test, y_test = X[test_idxs],y[test_idxs]\n",
    "\n",
    "#Delete the test set samples from X,y \n",
    "X = np.delete(X, test_idxs)\n",
    "y = np.delete(y, test_idxs, axis = 0)\n",
    "\n",
    "#usual train-val split. We use 0.11 here just match the test set size to validation set.\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af38b00e-bccb-4045-a2f6-7eb9f1fadbee",
   "metadata": {},
   "source": [
    "A quick check on the sample counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b4e7d3-dafd-43a4-9c63-ec96ef2e204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in Training set: 20025\n",
      "Samples in Validation set: 2475\n",
      "Samples in Test set: 2500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Samples in Training set: {x_train.shape[0]}\")\n",
    "print(f\"Samples in Validation set: {x_val.shape[0]}\")\n",
    "print(f\"Samples in Test set: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c59b12-3dee-4451-9853-0b70794a0ab2",
   "metadata": {},
   "source": [
    "A quick check for data imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "927babd6-b497-4b73-a491-b3f8f34b96cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([10085,  9940], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1211, 1289], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1204, 1271], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for i in [y_train, y_test, y_val]:\n",
    "    print(np.unique(i, return_counts = True, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7292f8-5cf5-4c9d-888d-1712fec486ad",
   "metadata": {},
   "source": [
    "We use the helper function to convert the data into tensorflow dataset objects. Note that , the __repeat__ flag needs to be set only for the train set , which by default is true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d5f8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The buid_dataset is a cutom function that returns tensor batches\n",
    "\n",
    "val_dataset=build_dataset(x_val,y_val,repeat=False,batch=256)\n",
    "test_dataset=build_dataset(x_test,y_test,repeat=False,batch=256)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "STEPS_PER_EPOCH=len(x_train)/BATCH_SIZE\n",
    "\n",
    "train_dataset=build_dataset(x_train,y_train,batch=BATCH_SIZE)\n",
    "input_shape=train_dataset.element_spec[0].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d224b67-e500-467b-a2d8-92b04cad264c",
   "metadata": {},
   "source": [
    "### Model Building\n",
    "\n",
    "This is quite standard. We use the helper functions to build a simple neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da7708bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 30, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,866\n",
      "Trainable params: 684,162\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=simple_model(input_shape)\n",
    "model.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = Adam(),\n",
    "        metrics = CategoricalAccuracy()\n",
    "    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24bec8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=ModelCheckpoint(filepath='model/model_full.h5',\n",
    "                           monitor='val_loss',save_best_only=True,verbose=1)\n",
    "\n",
    "csv_logger=keras.callbacks.CSVLogger('logger/trainlog_full.csv',\n",
    "                                     separator=',',append=False)\n",
    "\n",
    "early_stopper=keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            min_delta=0.001,\n",
    "                                            restore_best_weights=True,\n",
    "                                            patience=10)\n",
    "\n",
    "callbacks_list=[checkpoint,early_stopper,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.6914 - categorical_accuracy: 0.6475\n",
      "Epoch 1: val_loss improved from inf to 0.52715, saving model to model\\model_full.h5\n",
      "1251/1251 [==============================] - 78s 61ms/step - loss: 0.6914 - categorical_accuracy: 0.6475 - val_loss: 0.5272 - val_categorical_accuracy: 0.7317\n",
      "Epoch 2/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.5339 - categorical_accuracy: 0.7300\n",
      "Epoch 2: val_loss improved from 0.52715 to 0.52068, saving model to model\\model_full.h5\n",
      "1251/1251 [==============================] - 78s 62ms/step - loss: 0.5339 - categorical_accuracy: 0.7300 - val_loss: 0.5207 - val_categorical_accuracy: 0.7337\n",
      "Epoch 3/200\n",
      " 387/1251 [========>.....................] - ETA: 52s - loss: 0.4896 - categorical_accuracy: 0.7634"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=200,\n",
    "          validation_data=val_dataset,validation_steps=None,\n",
    "          callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89956a-fea4-434a-872c-cefc21a8278e",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "So we have trained a model using the full training set with 20000 samples. How does it perform on the test set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3af6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model/model_full.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-\" * 100)\n",
    "print(model.evaluate(test_dataset, verbose=0,return_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd70778-2bb7-45e5-9292-73daa85167cc",
   "metadata": {},
   "source": [
    "### Measuring Uncertainties\n",
    "\n",
    "In this section we evaluate the three metrics to measure uncertainty. We use the formula to find out the prediction probabilities of the 10 test samples with most uncertainty.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6060fe5-b1aa-49ad-bcd4-657869a6c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba = model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cea326-8f68-4b02-aad5-fa54c4e66d02",
   "metadata": {},
   "source": [
    "Now that we have the prediction probabilities of the entire test set, we can apply the formula to calculate the uncertainty metric and select the top 10 uncertain samples.<br> Let us start with Least Confidence or __LC__\n",
    "\n",
    "$$ LC_i = 1 - P_{imax}  $$\n",
    "\n",
    "P_imax is the maximum probability of the i_th sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed9e1be-843f-448f-bab3-eedda3e4d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Least Confidence\n",
    "y_test_uncert = 1 - y_test_proba.max(axis=1)\n",
    "#Indices of the top 10 Least Confidence\n",
    "y_test_top_lc = np.argsort(y_test_uncert)[-10:]\n",
    "#Print the predictions for the top 10 least confidence\n",
    "print(y_test_proba[y_test_top_lc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd6a00-2b3a-4ccc-9d42-c76f7e0c1c80",
   "metadata": {},
   "source": [
    "Margin of confidence of a sample is given by the 1st and 2nd highest prediction probability of a sample\n",
    "\n",
    "$$ MC_i = P_{i1} - P_{i2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737397c-a423-4e17-a85c-0687128b7677",
   "metadata": {},
   "outputs": [],
   "source": [
    "part = np.partition(-y_test_proba, 1, axis=1)\n",
    "# margin calculation\n",
    "margin = - part[:, 0] + part[:, 1]\n",
    "# indices of the lowest margin scores\n",
    "y_test_least_mc = np.argsort(margin)[:10]\n",
    "#Print the predictions for the 10 least margins\n",
    "print(y_test_proba[y_test_least_mc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4a9dc-6c49-4405-8186-a7fd67fba1c8",
   "metadata": {},
   "source": [
    "Finally entropy of the i_th sample is given by. Thankfully , we don't have to write the code for this calculation , as scipy provides a neat method called __entropy__ to do precisely that. \n",
    "\n",
    "$$ \\ E_{i} = \\sum_{n=1} p_{in} log   p_{in} \\ $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55311b0-217b-4cb2-949f-2df395545b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#indices of the predictions with 10 largest entropies\n",
    "y_test_max_ents = np.argsort(entropy(y_test_proba.T))[-10:]\n",
    "#Print the 10 predictions with largest entropies\n",
    "print(y_test_proba[y_test_max_ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

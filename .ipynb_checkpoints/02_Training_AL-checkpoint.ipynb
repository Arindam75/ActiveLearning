{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee49abea-fed4-48a4-8335-1d7a526195e3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246de192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.metrics import  Recall, CategoricalAccuracy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import concatenate as concat\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "\n",
    "#A custom library for helper functions\n",
    "from src.helper import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991cbbd-adaf-459d-bdd9-555950de6a5c",
   "metadata": {},
   "source": [
    "### Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afae67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.path.join(os.getcwd(),'train')\n",
    "label_dict={'cat':0,'dog':1}\n",
    "dataset=np.array([(os.path.join(path,i),label_dict[i.split('.')[0]]) for i in os.listdir(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a133272-8d8a-4eb1-b9d9-c5136cea4367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C:\\\\Users\\\\arind\\\\Documents\\\\Active_Learning\\\\dataset\\\\cat.0.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\Active_Learning\\\\dataset\\\\cat.1.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\Active_Learning\\\\dataset\\\\cat.10.jpg',\n",
       "        '0']], dtype='<U62')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977c450b-da5e-47f1-900c-bee9837f61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset[::,0],dataset[::,1]\n",
    "y = y.astype(int)\n",
    "y = to_categorical(y)\n",
    "\n",
    "#Shuffle the dataset\n",
    "p = np.random.permutation(len(X))\n",
    "X,y = X[p], y[p]\n",
    "\n",
    "#Strip off 10% samples for hold out test set\n",
    "test_idxs = np.random.choice(len(X), size=int(0.1*len(X)), replace=False, p=None)\n",
    "x_test, y_test = X[test_idxs],y[test_idxs]\n",
    "\n",
    "#Delete the test set samples from X,y \n",
    "X = np.delete(X, test_idxs)\n",
    "y = np.delete(y, test_idxs, axis = 0)\n",
    "\n",
    "#usual train-val split\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8163ee-7197-426c-8259-e48d49d045fc",
   "metadata": {},
   "source": [
    "In contrast to the previous notebook , in this experiment we will not use the full training set. Instead we will use only a subset of 7000 samples. Feel free to change the initial_seed yourself, say 3000. We will use a __seed__ dataset to build the initial model and keep the remaining samples in a __pool__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f75add-56a7-4178-b9bd-962657d4c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_seed = 7000\n",
    "x_seed , x_pool = x_train[0:initial_seed], x_train[initial_seed:]\n",
    "y_seed , y_pool = y_train[0:initial_seed], y_train[initial_seed:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b4e7d3-dafd-43a4-9c63-ec96ef2e204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in Seed set: 7000\n",
      "Samples in Pool: 13025\n",
      "Samples in Validation set: 2475\n",
      "Samples in Test set: 2500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Samples in Seed set: {x_seed.shape[0]}\")\n",
    "print(f\"Samples in Pool: {x_pool.shape[0]}\")\n",
    "print(f\"Samples in Validation set: {x_val.shape[0]}\")\n",
    "print(f\"Samples in Test set: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c59b12-3dee-4451-9853-0b70794a0ab2",
   "metadata": {},
   "source": [
    "A quick check for data imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927babd6-b497-4b73-a491-b3f8f34b96cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([3559, 3441], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([6526, 6499], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1211, 1289], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1204, 1271], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for i in [y_seed, y_pool, y_test, y_val]:\n",
    "    print(np.unique(i, return_counts = True, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99926145-21ac-4f38-9af7-51b26fe46ef5",
   "metadata": {},
   "source": [
    "We build the tensorflow dataset objects again. Note how the __train_dataset__ is now built from the seed set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5f8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The buid_dataset is a custom function that returns tensor batches\n",
    "\n",
    "val_dataset=build_dataset(x_val,y_val,repeat=False,batch=256)\n",
    "test_dataset=build_dataset(x_test,y_test,repeat=False,batch=256)\n",
    "pool_dataset=build_dataset(x_pool,y_pool,repeat=False,batch=256, shuffle = False)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "STEPS_PER_EPOCH=len(x_train)/BATCH_SIZE\n",
    "\n",
    "train_dataset=build_dataset(x_seed,y_seed,batch=BATCH_SIZE)\n",
    "input_shape=train_dataset.element_spec[0].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d224b67-e500-467b-a2d8-92b04cad264c",
   "metadata": {},
   "source": [
    "### Build Seed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7708bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 30, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,866\n",
      "Trainable params: 684,162\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=simple_model(input_shape)\n",
    "model.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = Adam(),\n",
    "        metrics= CategoricalAccuracy()\n",
    "    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24bec8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=ModelCheckpoint(filepath='model/model_al.h5',\n",
    "                           monitor='val_loss',save_best_only=True,verbose=1)\n",
    "\n",
    "csv_logger=keras.callbacks.CSVLogger('logger/trainlog_al.csv',\n",
    "                                     separator=',',append=False)\n",
    "\n",
    "early_stopper=keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            min_delta=0.001,\n",
    "                                            restore_best_weights=True,\n",
    "                                            patience=10)\n",
    "\n",
    "callbacks_list=[checkpoint,early_stopper,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc49bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.7039 - categorical_accuracy: 0.6353\n",
      "Epoch 1: val_loss improved from inf to 0.55950, saving model to model\\model_al.h5\n",
      "1251/1251 [==============================] - 16s 10ms/step - loss: 0.7039 - categorical_accuracy: 0.6352 - val_loss: 0.5595 - val_categorical_accuracy: 0.7127\n",
      "Epoch 2/200\n",
      "1248/1251 [============================>.] - ETA: 0s - loss: 0.5149 - categorical_accuracy: 0.7470\n",
      "Epoch 2: val_loss did not improve from 0.55950\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.5155 - categorical_accuracy: 0.7467 - val_loss: 0.6727 - val_categorical_accuracy: 0.7127\n",
      "Epoch 3/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.4459 - categorical_accuracy: 0.7907\n",
      "Epoch 3: val_loss did not improve from 0.55950\n",
      "1251/1251 [==============================] - 13s 10ms/step - loss: 0.4459 - categorical_accuracy: 0.7908 - val_loss: 0.7438 - val_categorical_accuracy: 0.6497\n",
      "Epoch 4/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.3842 - categorical_accuracy: 0.8269\n",
      "Epoch 4: val_loss improved from 0.55950 to 0.42460, saving model to model\\model_al.h5\n",
      "1251/1251 [==============================] - 15s 12ms/step - loss: 0.3841 - categorical_accuracy: 0.8270 - val_loss: 0.4246 - val_categorical_accuracy: 0.7996\n",
      "Epoch 5/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.3426 - categorical_accuracy: 0.8471\n",
      "Epoch 5: val_loss did not improve from 0.42460\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.3424 - categorical_accuracy: 0.8471 - val_loss: 0.5114 - val_categorical_accuracy: 0.7903\n",
      "Epoch 6/200\n",
      "1248/1251 [============================>.] - ETA: 0s - loss: 0.3238 - categorical_accuracy: 0.8595\n",
      "Epoch 6: val_loss improved from 0.42460 to 0.40107, saving model to model\\model_al.h5\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.3236 - categorical_accuracy: 0.8597 - val_loss: 0.4011 - val_categorical_accuracy: 0.8210\n",
      "Epoch 7/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.2795 - categorical_accuracy: 0.8801\n",
      "Epoch 7: val_loss did not improve from 0.40107\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2795 - categorical_accuracy: 0.8801 - val_loss: 0.8435 - val_categorical_accuracy: 0.6824\n",
      "Epoch 8/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2638 - categorical_accuracy: 0.8894\n",
      "Epoch 8: val_loss improved from 0.40107 to 0.39841, saving model to model\\model_al.h5\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2637 - categorical_accuracy: 0.8894 - val_loss: 0.3984 - val_categorical_accuracy: 0.8331\n",
      "Epoch 9/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2254 - categorical_accuracy: 0.9070\n",
      "Epoch 9: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 13s 11ms/step - loss: 0.2254 - categorical_accuracy: 0.9070 - val_loss: 0.5691 - val_categorical_accuracy: 0.7863\n",
      "Epoch 10/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.2081 - categorical_accuracy: 0.9155\n",
      "Epoch 10: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2083 - categorical_accuracy: 0.9154 - val_loss: 0.4417 - val_categorical_accuracy: 0.8186\n",
      "Epoch 11/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.2096 - categorical_accuracy: 0.9142\n",
      "Epoch 11: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.2097 - categorical_accuracy: 0.9140 - val_loss: 0.4168 - val_categorical_accuracy: 0.8287\n",
      "Epoch 12/200\n",
      "1248/1251 [============================>.] - ETA: 0s - loss: 0.1892 - categorical_accuracy: 0.9251\n",
      "Epoch 12: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.1892 - categorical_accuracy: 0.9251 - val_loss: 0.4475 - val_categorical_accuracy: 0.8251\n",
      "Epoch 13/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.1789 - categorical_accuracy: 0.9276\n",
      "Epoch 13: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.1789 - categorical_accuracy: 0.9276 - val_loss: 0.5388 - val_categorical_accuracy: 0.7923\n",
      "Epoch 14/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.1734 - categorical_accuracy: 0.9327\n",
      "Epoch 14: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.1734 - categorical_accuracy: 0.9327 - val_loss: 0.4171 - val_categorical_accuracy: 0.8295\n",
      "Epoch 15/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.1563 - categorical_accuracy: 0.9401\n",
      "Epoch 15: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.1565 - categorical_accuracy: 0.9399 - val_loss: 0.4342 - val_categorical_accuracy: 0.8307\n",
      "Epoch 16/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.1441 - categorical_accuracy: 0.9448\n",
      "Epoch 16: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.1441 - categorical_accuracy: 0.9448 - val_loss: 0.4609 - val_categorical_accuracy: 0.8291\n",
      "Epoch 17/200\n",
      "1249/1251 [============================>.] - ETA: 0s - loss: 0.1484 - categorical_accuracy: 0.9416\n",
      "Epoch 17: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.1486 - categorical_accuracy: 0.9414 - val_loss: 0.4988 - val_categorical_accuracy: 0.8295\n",
      "Epoch 18/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.1531 - categorical_accuracy: 0.9409\n",
      "Epoch 18: val_loss did not improve from 0.39841\n",
      "1251/1251 [==============================] - 14s 11ms/step - loss: 0.1530 - categorical_accuracy: 0.9409 - val_loss: 0.4417 - val_categorical_accuracy: 0.8364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2952a5a20d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=200,\n",
    "          validation_data=val_dataset,validation_steps=None,\n",
    "          callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89956a-fea4-434a-872c-cefc21a8278e",
   "metadata": {},
   "source": [
    "### Initial Model Evaluation on Test Dataset\n",
    "\n",
    "Now we have two models\n",
    "\n",
    "- An initial model trained on the seed dataset\n",
    "- A baseline model that we got from the 1st notebook , built on the entire training dataset.\n",
    "\n",
    "How do these two models perform on the test data ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3af6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model/model_al.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6fddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "{'loss': 0.38912251591682434, 'categorical_accuracy': 0.8339999914169312}\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 100)\n",
    "print(model.evaluate(test_dataset, verbose=0,return_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1becf2c-7c6d-4e42-81bc-e943bb266872",
   "metadata": {},
   "source": [
    "### Baseline Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b46257-08e9-4490-80d0-55b2446a2176",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 2\u001b[0m     model_full \u001b[38;5;241m=\u001b[39m \u001b[43mkeras\u001b[49m\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/model_full2.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     _, acc_baseline \u001b[38;5;241m=\u001b[39m model_full\u001b[38;5;241m.\u001b[39mevaluate(test_dataset)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    model_full = keras.models.load_model(\"model/model_full.h5\")\n",
    "    _, acc_baseline = model_full.evaluate(test_dataset)\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(model_full.evaluate(test_dataset, verbose=0,return_dict=True))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"model file model_full.h5 not found. Make sure to run 01_Training_Full.ipynb entirely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124b98f-39bd-4c98-a69b-a421ceeb86d6",
   "metadata": {},
   "source": [
    "So , the model built on an initial seed data is almost 10% behind in term of accuracy. Can we reach this baseline performance by incrementally querying samples from the pool.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca64c6f-ba35-477a-bc4b-6202d1a7e165",
   "metadata": {},
   "source": [
    "### Entering the AL Loop\n",
    "\n",
    "We will now iteratively query the pool for samples and add them to the seed set. Every time we can pick __sampling_size__ number of points from the pool with the largest entropy.\n",
    "\n",
    "- Step_1: test the current model on the test set. If it exceeds or equals the baseline accuracy , then we exit the AL loop. Otherwise we proceed.\n",
    "- Step_2: measure the uncertainties in the pool dataset. In other words we query the pool dataset. For this experiment, we will use the entropy measure. Pick the top 200 samples with maximum entropy, append them to the seed dataset and delete them from the pool.\n",
    "- Step_3: re-compile the model to reset the optimizer states and fit again. Save model if there is an improvement in loss. Go back to Step_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d1337ba-2e49-43a4-8bb3-c07332e1fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_size=200\n",
    "num_iterations = int(x_pool.shape[0]/sampling_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2dd52d0-4e88-415e-84fb-04734cbe7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_history = []\n",
    "csv_logger=keras.callbacks.CSVLogger('logger/trainlog_al.csv',\n",
    "                                 separator=',',append=True)\n",
    "callbacks_list=[checkpoint,early_stopper,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5125edd4-2cdc-430e-b60a-34718ffea5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy after 22 iteration 0.8871999979019165\n",
      "Terminating Training\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    #Step_1\n",
    "    loss, acc = model.evaluate(test_dataset, verbose=0)\n",
    "    print(f\"Test Set Accuracy after {iteration} iteration {acc}\")\n",
    "    al_history.append([loss, acc, x_seed.shape[0], x_pool.shape[0]])\n",
    "    if acc >= acc_baseline:\n",
    "        print(\"Terminating Training\")\n",
    "        break\n",
    "    \n",
    "    #Step_2\n",
    "    #Use the current model to predict the pool dataset\n",
    "    y_pool_proba = model.predict(pool_dataset)\n",
    "    \n",
    "    #Pick the index of the top entropy samples in pool\n",
    "    pool_max_ents = np.argsort(entropy(y_pool_proba.T))[-sampling_size:]\n",
    "    \n",
    "    #Acquire those samples from pool\n",
    "    x_sample = x_pool[pool_max_ents]\n",
    "    y_sample = y_pool[pool_max_ents]\n",
    "    \n",
    "    #Add these samples to the seed dataset\n",
    "    y_seed = concat((y_seed,y_sample),axis=0)\n",
    "    x_seed = concat((x_seed,x_sample),axis=0)\n",
    "     \n",
    "    #Delete the acquired samples from pool\n",
    "    x_pool = np.delete(x_pool, pool_max_ents, 0 )\n",
    "    y_pool = np.delete(y_pool, pool_max_ents, 0 )\n",
    "\n",
    "    #Build the tensorflow dataset object for this iteration\n",
    "    pool_dataset = build_dataset(x_pool,y_pool,repeat=False,batch=256,\n",
    "                                 shuffle = False)\n",
    "    train_dataset = build_dataset(x_seed,y_seed,batch=BATCH_SIZE) \n",
    "\n",
    "    print(f\"Samples in seed dataset {x_seed.shape[0]} , in pool dataset {x_pool.shape[0]}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    #Step_3\n",
    "    model.compile(\n",
    "        loss = \"binary_crossentropy\",\n",
    "        optimizer = Adam(),\n",
    "        metrics = CategoricalAccuracy()\n",
    "    )\n",
    "    \n",
    "    history = model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=100,\n",
    "          validation_data=val_dataset,validation_steps=None,\n",
    "          callbacks=callbacks_list)\n",
    "    \n",
    "    #If the fit method generated a new best model , load it for\n",
    "    #the next iteration\n",
    "    model = keras.models.load_model(\"model/model_al.h5\")\n",
    "    clear_output()\n",
    "    clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55fc8b8e-9c21-4a87-9ce9-0fd2bff7fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Pool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.389123</td>\n",
       "      <td>0.8340</td>\n",
       "      <td>7000</td>\n",
       "      <td>13025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.389123</td>\n",
       "      <td>0.8340</td>\n",
       "      <td>7200</td>\n",
       "      <td>12825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.368032</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>7400</td>\n",
       "      <td>12625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.387399</td>\n",
       "      <td>0.8452</td>\n",
       "      <td>7600</td>\n",
       "      <td>12425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.369284</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>7800</td>\n",
       "      <td>12225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.369284</td>\n",
       "      <td>0.8556</td>\n",
       "      <td>8000</td>\n",
       "      <td>12025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.343127</td>\n",
       "      <td>0.8576</td>\n",
       "      <td>8200</td>\n",
       "      <td>11825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.343127</td>\n",
       "      <td>0.8576</td>\n",
       "      <td>8400</td>\n",
       "      <td>11625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.343127</td>\n",
       "      <td>0.8576</td>\n",
       "      <td>8600</td>\n",
       "      <td>11425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.360890</td>\n",
       "      <td>0.8568</td>\n",
       "      <td>8800</td>\n",
       "      <td>11225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.349998</td>\n",
       "      <td>0.8664</td>\n",
       "      <td>9000</td>\n",
       "      <td>11025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.371800</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>9200</td>\n",
       "      <td>10825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>9400</td>\n",
       "      <td>10625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.343300</td>\n",
       "      <td>0.8736</td>\n",
       "      <td>9600</td>\n",
       "      <td>10425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.327718</td>\n",
       "      <td>0.8748</td>\n",
       "      <td>9800</td>\n",
       "      <td>10225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.320648</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>10000</td>\n",
       "      <td>10025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.320648</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>10200</td>\n",
       "      <td>9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.320648</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>10400</td>\n",
       "      <td>9625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.320648</td>\n",
       "      <td>0.8652</td>\n",
       "      <td>10600</td>\n",
       "      <td>9425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.314084</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>10800</td>\n",
       "      <td>9225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.314084</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>11000</td>\n",
       "      <td>9025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.313728</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>11200</td>\n",
       "      <td>8825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.301031</td>\n",
       "      <td>0.8872</td>\n",
       "      <td>11400</td>\n",
       "      <td>8625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test Loss  Test Accuracy   Seed   Pool\n",
       "0    0.389123         0.8340   7000  13025\n",
       "1    0.389123         0.8340   7200  12825\n",
       "2    0.368032         0.8376   7400  12625\n",
       "3    0.387399         0.8452   7600  12425\n",
       "4    0.369284         0.8556   7800  12225\n",
       "5    0.369284         0.8556   8000  12025\n",
       "6    0.343127         0.8576   8200  11825\n",
       "7    0.343127         0.8576   8400  11625\n",
       "8    0.343127         0.8576   8600  11425\n",
       "9    0.360890         0.8568   8800  11225\n",
       "10   0.349998         0.8664   9000  11025\n",
       "11   0.371800         0.8632   9200  10825\n",
       "12   0.343300         0.8736   9400  10625\n",
       "13   0.343300         0.8736   9600  10425\n",
       "14   0.327718         0.8748   9800  10225\n",
       "15   0.320648         0.8652  10000  10025\n",
       "16   0.320648         0.8652  10200   9825\n",
       "17   0.320648         0.8652  10400   9625\n",
       "18   0.320648         0.8652  10600   9425\n",
       "19   0.314084         0.8772  10800   9225\n",
       "20   0.314084         0.8772  11000   9025\n",
       "21   0.313728         0.8744  11200   8825\n",
       "22   0.301031         0.8872  11400   8625"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(al_history, columns = ['Test Loss', 'Test Accuracy', 'Seed', 'Pool'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbfb6097-411c-4502-a18c-57c633fe01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('logger/AL_tracking.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d3388e-cc57-4dec-9436-06b32ddb2edf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

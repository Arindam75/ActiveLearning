{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee49abea-fed4-48a4-8335-1d7a526195e3",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246de192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.metrics import  Recall, CategoricalAccuracy\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import concatenate as concat\n",
    "from scipy.stats import entropy\n",
    "import os\n",
    "\n",
    "#A custom library for helper functions\n",
    "from src.helper import *\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991cbbd-adaf-459d-bdd9-555950de6a5c",
   "metadata": {},
   "source": [
    "### Build Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afae67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=os.path.join(os.getcwd(),'train')\n",
    "label_dict={'cat':0,'dog':1}\n",
    "dataset=np.array([(os.path.join(path,i),label_dict[i.split('.')[0]]) for i in os.listdir(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a133272-8d8a-4eb1-b9d9-c5136cea4367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['C:\\\\Users\\\\arind\\\\Documents\\\\ActiveLearning\\\\train\\\\cat.0.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\ActiveLearning\\\\train\\\\cat.1.jpg',\n",
       "        '0'],\n",
       "       ['C:\\\\Users\\\\arind\\\\Documents\\\\ActiveLearning\\\\train\\\\cat.10.jpg',\n",
       "        '0']], dtype='<U59')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977c450b-da5e-47f1-900c-bee9837f61b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset[::,0],dataset[::,1]\n",
    "y = y.astype(int)\n",
    "y = to_categorical(y)\n",
    "\n",
    "#Shuffle the dataset\n",
    "p = np.random.permutation(len(X))\n",
    "X,y = X[p], y[p]\n",
    "\n",
    "#Strip off 10% samples for hold out test set\n",
    "test_idxs = np.random.choice(len(X), size=int(0.1*len(X)), replace=False, p=None)\n",
    "x_test, y_test = X[test_idxs],y[test_idxs]\n",
    "\n",
    "#Delete the test set samples from X,y \n",
    "X = np.delete(X, test_idxs)\n",
    "y = np.delete(y, test_idxs, axis = 0)\n",
    "\n",
    "#usual train-val split\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.11, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8163ee-7197-426c-8259-e48d49d045fc",
   "metadata": {},
   "source": [
    "In contrast to the previous notebook , in this experiment we will not use the full training set. Instead we will use only a subset of 7000 samples. Feel free to change the initial_seed yourself, say 3000. We will use a __seed__ dataset to build the initial model and keep the remaining samples in a __pool__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3f75add-56a7-4178-b9bd-962657d4c474",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_seed = 5000\n",
    "x_seed , x_pool = x_train[0:initial_seed], x_train[initial_seed:]\n",
    "y_seed , y_pool = y_train[0:initial_seed], y_train[initial_seed:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b4e7d3-dafd-43a4-9c63-ec96ef2e204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples in Seed set: 5000\n",
      "Samples in Pool: 15025\n",
      "Samples in Validation set: 2475\n",
      "Samples in Test set: 2500\n"
     ]
    }
   ],
   "source": [
    "print(f\"Samples in Seed set: {x_seed.shape[0]}\")\n",
    "print(f\"Samples in Pool: {x_pool.shape[0]}\")\n",
    "print(f\"Samples in Validation set: {x_val.shape[0]}\")\n",
    "print(f\"Samples in Test set: {x_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c59b12-3dee-4451-9853-0b70794a0ab2",
   "metadata": {},
   "source": [
    "A quick check for data imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "927babd6-b497-4b73-a491-b3f8f34b96cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([2533, 2467], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([7552, 7473], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1211, 1289], dtype=int64))\n",
      "(array([[0., 1.],\n",
      "       [1., 0.]], dtype=float32), array([1204, 1271], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "for i in [y_seed, y_pool, y_test, y_val]:\n",
    "    print(np.unique(i, return_counts = True, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99926145-21ac-4f38-9af7-51b26fe46ef5",
   "metadata": {},
   "source": [
    "We build the tensorflow dataset objects again. Note how the __train_dataset__ is now built from the seed set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d5f8544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The buid_dataset is a custom function that returns tensor batches\n",
    "\n",
    "val_dataset=build_dataset(x_val,y_val,repeat=False,batch=256)\n",
    "test_dataset=build_dataset(x_test,y_test,repeat=False,batch=256)\n",
    "pool_dataset=build_dataset(x_pool,y_pool,repeat=False,batch=256, shuffle = False)\n",
    "\n",
    "BATCH_SIZE=16\n",
    "STEPS_PER_EPOCH=len(x_train)/BATCH_SIZE\n",
    "\n",
    "train_dataset=build_dataset(x_seed,y_seed,batch=BATCH_SIZE)\n",
    "input_shape=train_dataset.element_spec[0].shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d224b67-e500-467b-a2d8-92b04cad264c",
   "metadata": {},
   "source": [
    "### Build Seed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da7708bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64, 64, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 30, 30, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 13, 13, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 13, 13, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               589952    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 684,866\n",
      "Trainable params: 684,162\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=simple_model(input_shape)\n",
    "model.compile(\n",
    "        loss = \"categorical_crossentropy\",\n",
    "        optimizer = Adam(),\n",
    "        metrics= CategoricalAccuracy()\n",
    "    )\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24bec8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint=ModelCheckpoint(filepath='model/model_al.h5',\n",
    "                           monitor='val_loss',save_best_only=True,verbose=1)\n",
    "\n",
    "csv_logger=keras.callbacks.CSVLogger('logger/trainlog_al.csv',\n",
    "                                     separator=',',append=False)\n",
    "\n",
    "early_stopper=keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                            min_delta=0.001,\n",
    "                                            restore_best_weights=True,\n",
    "                                            patience=10)\n",
    "\n",
    "callbacks_list=[checkpoint,early_stopper,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc49bc38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.6793 - categorical_accuracy: 0.6573\n",
      "Epoch 1: val_loss improved from inf to 0.55966, saving model to model\\model_al.h5\n",
      "1251/1251 [==============================] - 51s 28ms/step - loss: 0.6790 - categorical_accuracy: 0.6575 - val_loss: 0.5597 - val_categorical_accuracy: 0.6986\n",
      "Epoch 2/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.5017 - categorical_accuracy: 0.7545\n",
      "Epoch 2: val_loss did not improve from 0.55966\n",
      "1251/1251 [==============================] - 33s 27ms/step - loss: 0.5017 - categorical_accuracy: 0.7546 - val_loss: 0.6272 - val_categorical_accuracy: 0.7487\n",
      "Epoch 3/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.4254 - categorical_accuracy: 0.8022\n",
      "Epoch 3: val_loss improved from 0.55966 to 0.43005, saving model to model\\model_al.h5\n",
      "1251/1251 [==============================] - 33s 27ms/step - loss: 0.4253 - categorical_accuracy: 0.8022 - val_loss: 0.4301 - val_categorical_accuracy: 0.8004\n",
      "Epoch 4/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.3590 - categorical_accuracy: 0.8411\n",
      "Epoch 4: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 33s 27ms/step - loss: 0.3590 - categorical_accuracy: 0.8411 - val_loss: 0.5228 - val_categorical_accuracy: 0.7790\n",
      "Epoch 5/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.3024 - categorical_accuracy: 0.8691\n",
      "Epoch 5: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 33s 27ms/step - loss: 0.3024 - categorical_accuracy: 0.8691 - val_loss: 0.4863 - val_categorical_accuracy: 0.7939\n",
      "Epoch 6/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.2753 - categorical_accuracy: 0.8855\n",
      "Epoch 6: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 34s 27ms/step - loss: 0.2753 - categorical_accuracy: 0.8855 - val_loss: 0.4434 - val_categorical_accuracy: 0.8048\n",
      "Epoch 7/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.2360 - categorical_accuracy: 0.9023\n",
      "Epoch 7: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 34s 27ms/step - loss: 0.2359 - categorical_accuracy: 0.9023 - val_loss: 0.4348 - val_categorical_accuracy: 0.8214\n",
      "Epoch 8/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.2214 - categorical_accuracy: 0.9100\n",
      "Epoch 8: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 34s 27ms/step - loss: 0.2214 - categorical_accuracy: 0.9100 - val_loss: 0.4390 - val_categorical_accuracy: 0.8255\n",
      "Epoch 9/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.1749 - categorical_accuracy: 0.9317\n",
      "Epoch 9: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 33s 27ms/step - loss: 0.1749 - categorical_accuracy: 0.9317 - val_loss: 0.5380 - val_categorical_accuracy: 0.8057\n",
      "Epoch 10/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.1894 - categorical_accuracy: 0.9270\n",
      "Epoch 10: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 33s 26ms/step - loss: 0.1893 - categorical_accuracy: 0.9271 - val_loss: 0.4726 - val_categorical_accuracy: 0.8113\n",
      "Epoch 11/200\n",
      "1250/1251 [============================>.] - ETA: 0s - loss: 0.1574 - categorical_accuracy: 0.9395\n",
      "Epoch 11: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 33s 26ms/step - loss: 0.1573 - categorical_accuracy: 0.9395 - val_loss: 0.5074 - val_categorical_accuracy: 0.8279\n",
      "Epoch 12/200\n",
      "1252/1251 [==============================] - ETA: 0s - loss: 0.1524 - categorical_accuracy: 0.9405\n",
      "Epoch 12: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 33s 27ms/step - loss: 0.1524 - categorical_accuracy: 0.9405 - val_loss: 0.5108 - val_categorical_accuracy: 0.8149\n",
      "Epoch 13/200\n",
      "1251/1251 [============================>.] - ETA: 0s - loss: 0.1513 - categorical_accuracy: 0.9433\n",
      "Epoch 13: val_loss did not improve from 0.43005\n",
      "1251/1251 [==============================] - 33s 26ms/step - loss: 0.1513 - categorical_accuracy: 0.9433 - val_loss: 0.4909 - val_categorical_accuracy: 0.8214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fa8d9474f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=200,\n",
    "          validation_data=val_dataset,validation_steps=None,\n",
    "          callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b89956a-fea4-434a-872c-cefc21a8278e",
   "metadata": {},
   "source": [
    "### Initial Model Evaluation on Test Dataset\n",
    "\n",
    "Now we have two models\n",
    "\n",
    "- An initial model trained on the seed dataset\n",
    "- A baseline model that we got from the 1st notebook , built on the entire training dataset.\n",
    "\n",
    "How do these two models perform on the test data ?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a3af6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"model/model_al.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e6fddf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "{'loss': 0.47423428297042847, 'categorical_accuracy': 0.7764000296592712}\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 100)\n",
    "print(model.evaluate(test_dataset, verbose=0,return_dict=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1becf2c-7c6d-4e42-81bc-e943bb266872",
   "metadata": {},
   "source": [
    "### Baseline Model Evaluation on Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1b46257-08e9-4490-80d0-55b2446a2176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 87ms/step - loss: 0.2883 - categorical_accuracy: 0.8832\n",
      "----------------------------------------------------------------------------------------------------\n",
      "{'loss': 0.2882635295391083, 'categorical_accuracy': 0.8831999897956848}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    model_full = keras.models.load_model(\"model/model_baseline.h5\")\n",
    "    _, acc_baseline = model_full.evaluate(test_dataset)\n",
    "\n",
    "    print(\"-\" * 100)\n",
    "    print(model_full.evaluate(test_dataset, verbose=0,return_dict=True))\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"model file model_baseline.h5 not found. Make sure to run 01_Training_Full.ipynb entirely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124b98f-39bd-4c98-a69b-a421ceeb86d6",
   "metadata": {},
   "source": [
    "So , the model built on an initial seed data is almost 5% behind in term of accuracy. Can we reach this baseline performance by incrementally querying samples from the pool.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca64c6f-ba35-477a-bc4b-6202d1a7e165",
   "metadata": {},
   "source": [
    "### Entering the AL Loop\n",
    "\n",
    "We will now iteratively query the pool for samples and add them to the seed set. Every time we can pick __sampling_size__ number of points from the pool with the largest entropy.\n",
    "\n",
    "- Step_1: test the current model on the test set. If it exceeds or equals the baseline accuracy , then we exit the AL loop. Otherwise we proceed.\n",
    "- Step_2: measure the uncertainties in the pool dataset. In other words we query the pool dataset. For this experiment, we will use the entropy measure. Pick the top 200 samples with maximum entropy, append them to the seed dataset and delete them from the pool.\n",
    "- Step_3: re-compile the model to reset the optimizer states and fit again. Save model if there is an improvement in loss. Go back to Step_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ca0d13-c2c9-403a-b5d0-8cde50e30ef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8832"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_baseline = round(acc_baseline,4)\n",
    "acc_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d1337ba-2e49-43a4-8bb3-c07332e1fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_size=200\n",
    "num_iterations = int(x_pool.shape[0]/sampling_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2dd52d0-4e88-415e-84fb-04734cbe7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_history = []\n",
    "csv_logger=keras.callbacks.CSVLogger('logger/trainlog_al.csv',\n",
    "                                 separator=',',append=True)\n",
    "callbacks_list=[checkpoint,early_stopper,csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5125edd4-2cdc-430e-b60a-34718ffea5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy after 29 iteration 0.8823999762535095\n",
      "Terminating Training\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(num_iterations):\n",
    "    \n",
    "    #Step_1\n",
    "    loss, acc = model.evaluate(test_dataset, verbose=0)\n",
    "    print(f\"Test Set Accuracy after {iteration} iteration {acc}\")\n",
    "    al_history.append([loss, acc, x_seed.shape[0], x_pool.shape[0]])\n",
    "    \n",
    "    if acc_baseline - 0.0025 < acc <= acc_baseline + 0.0025:\n",
    "        print(\"Terminating Training\")\n",
    "        break\n",
    "    \n",
    "    #Step_2\n",
    "    #Use the current model to predict the pool dataset\n",
    "    print(\"Predicting pool dataset\")\n",
    "    y_pool_proba = model.predict(pool_dataset)\n",
    "    \n",
    "    #Pick the index of the top entropy samples in pool\n",
    "    pool_max_ents = np.argsort(entropy(y_pool_proba.T))[-sampling_size:]\n",
    "    \n",
    "    #Acquire those samples from pool\n",
    "    x_sample = x_pool[pool_max_ents]\n",
    "    y_sample = y_pool[pool_max_ents]\n",
    "    \n",
    "    #Add these samples to the seed dataset\n",
    "    y_seed = concat((y_seed,y_sample),axis=0)\n",
    "    x_seed = concat((x_seed,x_sample),axis=0)\n",
    "     \n",
    "    #Delete the acquired samples from pool\n",
    "    x_pool = np.delete(x_pool, pool_max_ents, 0 )\n",
    "    y_pool = np.delete(y_pool, pool_max_ents, 0 )\n",
    "\n",
    "    #Build the tensorflow dataset object for this iteration\n",
    "    pool_dataset = build_dataset(x_pool,y_pool,repeat=False,batch=256,\n",
    "                                 shuffle = False)\n",
    "    train_dataset = build_dataset(x_seed,y_seed,batch=BATCH_SIZE) \n",
    "\n",
    "    print(f\"Samples in seed dataset {x_seed.shape[0]} , in pool dataset {x_pool.shape[0]}\")\n",
    "    print(\"-\" * 100)\n",
    "\n",
    "    #Step_3\n",
    "    model.compile(\n",
    "        loss = \"binary_crossentropy\",\n",
    "        optimizer = Adam(),\n",
    "        metrics = CategoricalAccuracy()\n",
    "    )\n",
    "    \n",
    "    history = model.fit(train_dataset,steps_per_epoch=STEPS_PER_EPOCH,epochs=100,\n",
    "          validation_data=val_dataset,validation_steps=None,\n",
    "          callbacks=callbacks_list)\n",
    "    \n",
    "    #If the fit method generated a new best model , load it for\n",
    "    #the next iteration\n",
    "    model = keras.models.load_model(\"model/model_al.h5\")\n",
    "    clear_output()\n",
    "    clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55fc8b8e-9c21-4a87-9ce9-0fd2bff7fc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Loss</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Seed</th>\n",
       "      <th>Pool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.474234</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>5000</td>\n",
       "      <td>15025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.474234</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>5200</td>\n",
       "      <td>14825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.474234</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>5400</td>\n",
       "      <td>14625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.474234</td>\n",
       "      <td>0.7764</td>\n",
       "      <td>5600</td>\n",
       "      <td>14425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.396395</td>\n",
       "      <td>0.8288</td>\n",
       "      <td>5800</td>\n",
       "      <td>14225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.396396</td>\n",
       "      <td>0.8288</td>\n",
       "      <td>6000</td>\n",
       "      <td>14025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.412892</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>6200</td>\n",
       "      <td>13825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.412892</td>\n",
       "      <td>0.8260</td>\n",
       "      <td>6400</td>\n",
       "      <td>13625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.411293</td>\n",
       "      <td>0.8256</td>\n",
       "      <td>6600</td>\n",
       "      <td>13425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.408231</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>6800</td>\n",
       "      <td>13225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.408231</td>\n",
       "      <td>0.8376</td>\n",
       "      <td>7000</td>\n",
       "      <td>13025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.388823</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>7200</td>\n",
       "      <td>12825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.388823</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>7400</td>\n",
       "      <td>12625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.388823</td>\n",
       "      <td>0.8424</td>\n",
       "      <td>7600</td>\n",
       "      <td>12425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.376361</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>7800</td>\n",
       "      <td>12225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.376361</td>\n",
       "      <td>0.8564</td>\n",
       "      <td>8000</td>\n",
       "      <td>12025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.357412</td>\n",
       "      <td>0.8624</td>\n",
       "      <td>8200</td>\n",
       "      <td>11825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.342962</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>8400</td>\n",
       "      <td>11625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.342962</td>\n",
       "      <td>0.8572</td>\n",
       "      <td>8600</td>\n",
       "      <td>11425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.343382</td>\n",
       "      <td>0.8616</td>\n",
       "      <td>8800</td>\n",
       "      <td>11225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.332257</td>\n",
       "      <td>0.8620</td>\n",
       "      <td>9000</td>\n",
       "      <td>11025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.332257</td>\n",
       "      <td>0.8620</td>\n",
       "      <td>9200</td>\n",
       "      <td>10825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.332257</td>\n",
       "      <td>0.8620</td>\n",
       "      <td>9400</td>\n",
       "      <td>10625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.332257</td>\n",
       "      <td>0.8620</td>\n",
       "      <td>9600</td>\n",
       "      <td>10425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.331334</td>\n",
       "      <td>0.8628</td>\n",
       "      <td>9800</td>\n",
       "      <td>10225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.323658</td>\n",
       "      <td>0.8640</td>\n",
       "      <td>10000</td>\n",
       "      <td>10025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.320844</td>\n",
       "      <td>0.8744</td>\n",
       "      <td>10200</td>\n",
       "      <td>9825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.301905</td>\n",
       "      <td>0.8700</td>\n",
       "      <td>10400</td>\n",
       "      <td>9625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.314669</td>\n",
       "      <td>0.8764</td>\n",
       "      <td>10600</td>\n",
       "      <td>9425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.313196</td>\n",
       "      <td>0.8824</td>\n",
       "      <td>10800</td>\n",
       "      <td>9225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Test Loss  Test Accuracy   Seed   Pool\n",
       "0    0.474234         0.7764   5000  15025\n",
       "1    0.474234         0.7764   5200  14825\n",
       "2    0.474234         0.7764   5400  14625\n",
       "3    0.474234         0.7764   5600  14425\n",
       "4    0.396395         0.8288   5800  14225\n",
       "5    0.396396         0.8288   6000  14025\n",
       "6    0.412892         0.8260   6200  13825\n",
       "7    0.412892         0.8260   6400  13625\n",
       "8    0.411293         0.8256   6600  13425\n",
       "9    0.408231         0.8376   6800  13225\n",
       "10   0.408231         0.8376   7000  13025\n",
       "11   0.388823         0.8424   7200  12825\n",
       "12   0.388823         0.8424   7400  12625\n",
       "13   0.388823         0.8424   7600  12425\n",
       "14   0.376361         0.8564   7800  12225\n",
       "15   0.376361         0.8564   8000  12025\n",
       "16   0.357412         0.8624   8200  11825\n",
       "17   0.342962         0.8572   8400  11625\n",
       "18   0.342962         0.8572   8600  11425\n",
       "19   0.343382         0.8616   8800  11225\n",
       "20   0.332257         0.8620   9000  11025\n",
       "21   0.332257         0.8620   9200  10825\n",
       "22   0.332257         0.8620   9400  10625\n",
       "23   0.332257         0.8620   9600  10425\n",
       "24   0.331334         0.8628   9800  10225\n",
       "25   0.323658         0.8640  10000  10025\n",
       "26   0.320844         0.8744  10200   9825\n",
       "27   0.301905         0.8700  10400   9625\n",
       "28   0.314669         0.8764  10600   9425\n",
       "29   0.313196         0.8824  10800   9225"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(al_history, columns = ['Test Loss', 'Test Accuracy', 'Seed', 'Pool'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbfb6097-411c-4502-a18c-57c633fe01ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('logger/AL_tracking.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c06ce2-f6fe-4580-8723-026243a7fb10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
